# GENERATIVE-TEXT-MODEL


# TASK DESCRIPTION:
This project uses the GPT-2 model to generate text based on a user-provided topic. By leveraging the Hugging Face transformers library, the pre-trained GPT-2 model is used to produce coherent and contextually relevant text. The user inputs a topic, and the model generates a response with control over the length, randomness, and repetition. This simple implementation showcases the power of GPT-2 in natural language generation tasks.
